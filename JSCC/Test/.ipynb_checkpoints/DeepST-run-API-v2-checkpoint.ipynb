{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "248b1169-3121-4a76-9d69-99000549d60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.10.104:5555/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [11/Mar/2023 18:40:17] \"GET /translate HTTP/1.1\" 405 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.43it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 18:41:28] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 18:41:28] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 25.90it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 18:42:11] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 18:42:11] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.95it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 18:42:55] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 18:42:55] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 37.51it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 18:43:39] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 18:43:39] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 43.77it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 18:43:46] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 18:43:46] \"GET /results HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:00:54] \"GET /translate HTTP/1.1\" 405 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 92.52it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:01:54] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:01:54] \"GET /results HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:02:00] \"POST /results HTTP/1.1\" 405 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 42.22it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:03:07] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:03:07] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 136.52it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:06:04] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:06:04] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 99.39it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:09:52] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:09:52] \"GET /results HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:09:55] \"POST /results HTTP/1.1\" 405 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 139.78it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:10:06] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:10:06] \"GET /results HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:10:10] \"POST /results HTTP/1.1\" 405 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 136.07it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:13:02] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:13:02] \"GET /results HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:13:06] \"POST /results HTTP/1.1\" 405 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:19:01] \"POST /results HTTP/1.1\" 405 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:19:04] \"POST /results HTTP/1.1\" 405 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 141.74it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:19:18] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:19:18] \"GET /results HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:19:22] \"POST /results HTTP/1.1\" 405 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 138.99it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:08] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:08] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 136.56it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:11] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:11] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 138.49it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:14] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:14] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 142.94it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:17] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:17] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 130.00it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:20] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:20] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 82.28it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:28] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:28] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 99.92it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:34] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:21:34] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 56.93it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:23:20] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:23:20] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:23:52] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:23:52] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.05it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:24:27] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:24:27] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.09it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:24:53] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:24:53] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.92it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:25:42] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:25:42] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:25:52] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:25:52] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.63it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:26:17] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:26:17] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.55it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:26:34] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:26:34] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 18.38it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:26:38] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:26:38] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:27:11] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:27:11] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.85it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:27:33] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:27:33] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:28:52] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:28:52] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:29:12] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:29:12] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:31:15] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:31:15] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.29it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:31:32] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:31:32] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 50.36it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:32:02] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:32:02] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 38.99it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:34:05] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:34:05] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 37.75it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:34:05] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:34:05] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 39.65it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:34:05] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:34:05] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 35.79it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:34:05] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:34:05] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 16.14it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:35:01] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:35:01] \"GET /results HTTP/1.1\" 200 -\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 22.06it/s]\n",
      "127.0.0.1 - - [11/Mar/2023 22:35:59] \"POST /translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2023 22:35:59] \"GET /results HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os  # use OS dependent functions like reading or writing files\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '10' # default is 8, Mac Book M1 Pro has 10 can support 10, which may slow down the computer \n",
    "import os.path\n",
    "from os import path\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "\n",
    "import string\n",
    "\n",
    "import datetime\n",
    "import re  # regular expressions in Python for searching patterns\n",
    "\n",
    "import numpy as np\n",
    "import collections # compute source_vocab\n",
    "from tqdm import tqdm \n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction \n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "#from translate import translate\n",
    "import threading\n",
    "from multiprocessing import Process\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "import traceback\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Config:\n",
    "    E = 128\n",
    "    V = 128\n",
    "    sigma = 0.1   \n",
    "    num_steps = 40 \n",
    "    checkpoint_path = \"model_checkpoint.pth\"\n",
    "    corpus_train_vocab = \"corpus_train_vocab.txt\"\n",
    "    corpus_train_split = \"corpus_train_split.txt\"\n",
    "    input_file_path = \"test.txt\"\n",
    "    output_file_path = \"data/output.txt\"\n",
    "    output_log = \"data/log.txt\"\n",
    "    checkpoint_path = \"model_checkpoint.pth\"\n",
    "    batch_size = 32\n",
    "CONFIG = Config()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Vocab:  \n",
    "    # __init__ sorts the tokens by frequency in descending order, and assigns an index to each token\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): \n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = tokens\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],  \n",
    "                                   reverse=True)            # A list of tokens sorted by frequency\n",
    "        # Unknown tokens have an index of 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens  # List, idx corresponds to the position of token\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "        self.stoi = self.token_to_idx\n",
    "        self.itos = self.idx_to_token\n",
    "        self.tokens = self.idx_to_token[1:]\n",
    "\n",
    "    def __len__(self): # returns the length of the vocabulary\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens): # returns the index of a given token\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices): # returns the token corresponding to a given index\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "    def to_indices(self, tokens): # returns the index corresponding to a given token\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.token_to_idx.get(token, self.unk) for token in tokens]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Unknown tokens have an index of 0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self): # returns the list of token frequencies that was sorted in descending order during initialization of the Vocab class.\n",
    "        return self._token_freqs\n",
    "\n",
    "## load here to allow dataset module to use the source_vocab file\n",
    "checkpoint = torch.load(CONFIG.checkpoint_path, map_location=torch.device('cpu'))\n",
    "source_vocab = checkpoint['source_vocab']\n",
    "train_corpus = open(CONFIG.corpus_train_split, 'r').readlines()  # the special \"\\n\" has not been processed\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):  # define Pytorch data set\n",
    "    def __init__(self, vocab=source_vocab, corpus=train_corpus):\n",
    "        self.vocab = vocab\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.corpus[index].strip().split() + ['<eos>']\n",
    "        return self.vocab[sentence], len(sentence)\n",
    "    \n",
    "\n",
    "def collate_fn(batch_data): \n",
    "    batch_data.sort(key=lambda xi: len(xi[0]), reverse=True)\n",
    "    data_length = [xi[1] for xi in batch_data]\n",
    "    data = [torch.tensor(xi[0]) for xi in batch_data]\n",
    "    padded_data = nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=1)\n",
    "    return padded_data, torch.tensor(data_length)\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"position encoding\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "class MyEmbedding(nn.Module):\n",
    "    def __init__(self, vocab=source_vocab):\n",
    "        super(MyEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), CONFIG.E, padding_idx=vocab['<pad>'])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.embedding(X)\n",
    "\n",
    "# with batch normalization\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab=source_vocab):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.position_encoding = PositionalEncoding(CONFIG.V, dropout=0.1)\n",
    "        self.norm1 = nn.BatchNorm1d(CONFIG.V)\n",
    "        self.transformer_encoder1 = nn.TransformerEncoderLayer(d_model=CONFIG.V, nhead=8, dim_feedforward=512,  # other's parameter\n",
    "                                                              batch_first=True)\n",
    "        self.norm2 = nn.BatchNorm1d(CONFIG.V)\n",
    "        self.transformer_encoder2 = nn.TransformerEncoderLayer(d_model=CONFIG.V, nhead=8, dim_feedforward=512,  # other's parameter\n",
    "                                                              batch_first=True)\n",
    "        self.norm3 = nn.BatchNorm1d(CONFIG.V)\n",
    "        self.transformer_encoder3 = nn.TransformerEncoderLayer(d_model=CONFIG.V, nhead=8, dim_feedforward=512,  # other's parameter\n",
    "                                                              batch_first=True)\n",
    "        self.linear1 = nn.Linear(CONFIG.V, 2 * CONFIG.V)\n",
    "        self.norm4 = nn.BatchNorm1d(2 * CONFIG.V)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(2 * CONFIG.V, 16)\n",
    "        self.norm5 = nn.BatchNorm1d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X, valid_lens):\n",
    "        mask = (torch.arange((X.shape[1]), device=device).unsqueeze(0) >= valid_lens.unsqueeze(1)).to(device)\n",
    "        X2 = self.position_encoding(X)\n",
    "        X2_norm = self.norm1(X2.transpose(1, 2)).transpose(1, 2)\n",
    "        X3 = self.transformer_encoder1(X2_norm, src_key_padding_mask=mask)\n",
    "        X3_norm = self.norm2(X3.transpose(1, 2)).transpose(1, 2)\n",
    "        X4 = self.transformer_encoder2(X3_norm, src_key_padding_mask=mask)\n",
    "        X4_norm = self.norm3(X4.transpose(1, 2)).transpose(1, 2)\n",
    "        X5 = self.transformer_encoder3(X4_norm, src_key_padding_mask=mask)\n",
    "        X6 = self.linear1(X5)\n",
    "        X6_norm = self.norm4(X6.transpose(1, 2)).transpose(1, 2)\n",
    "        X7 = self.relu1(X6_norm)\n",
    "        X8 = self.linear2(X7)\n",
    "        X8_norm = self.norm5(X8.transpose(1, 2)).transpose(1, 2)\n",
    "        X9 = self.relu2(X8_norm)\n",
    "        return X9\n",
    "        \n",
    "def Channel(X):  # AWGN\n",
    "    return X + torch.normal(0, CONFIG.sigma, size=X.shape).to(device)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab=source_vocab):\n",
    "        super(Decoder, self).__init__()\n",
    "        # reshape\n",
    "        self.position_encoding = PositionalEncoding(CONFIG.V, dropout=0.1)\n",
    "        self.linear1 = nn.Linear(16, 2 * CONFIG.V)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(2 * CONFIG.V, CONFIG.V)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.transformer_decoder1 = nn.TransformerDecoderLayer(d_model=CONFIG.V, nhead=8, dim_feedforward=512,  # other's parameter\n",
    "                                                              batch_first=True)\n",
    "        self.transformer_decoder2 = nn.TransformerDecoderLayer(d_model=CONFIG.V, nhead=8, dim_feedforward=512,  # other's parameter\n",
    "                                                              batch_first=True)\n",
    "        self.transformer_decoder3 = nn.TransformerDecoderLayer(d_model=CONFIG.V, nhead=8, dim_feedforward=512,  # other's parameter\n",
    "                                                              batch_first=True)\n",
    "        self.linear3 = nn.Linear(CONFIG.V, len(source_vocab))\n",
    "        \n",
    "    def forward(self, emb_decoder_input, channel_output, origin_len, tgt_mask=None, mode='train'):\n",
    "        memory_mask = (torch.arange((channel_output.shape[1]), dtype=torch.float32,\n",
    "                            device=device)[None, :] >= origin_len[:, None]).to(device)\n",
    "        channel_output = self.linear1(channel_output)\n",
    "        channel_output = self.relu1(channel_output)\n",
    "        channel_output = self.linear2(channel_output)\n",
    "        memory = self.relu2(channel_output)\n",
    "        emb_decoder_input = self.position_encoding(emb_decoder_input)\n",
    "        X6 = self.transformer_decoder1(emb_decoder_input, memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_mask, tgt_key_padding_mask=memory_mask if mode == 'train' else None)\n",
    "        X7 = self.transformer_decoder2(X6, memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_mask, tgt_key_padding_mask=memory_mask if mode == 'train' else None)\n",
    "        X8 = self.transformer_decoder3(X7, memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_mask, tgt_key_padding_mask=memory_mask if mode == 'train' else None)\n",
    "        X9 = self.linear3(X8)\n",
    "        return X9\n",
    "                \n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_dim=16, out_dim=64, latent_dim=8):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(out_dim, latent_dim)\n",
    "        self.logvar = nn.Linear(out_dim, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, in_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu(h)\n",
    "        logvar = self.logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, x.shape[-1]))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "class DeepST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepST, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.channel = Channel\n",
    "        self.decoder = Decoder()\n",
    "        self.vae = VAE(in_dim=16, out_dim=64) # change out_dim 16 to 64 for example\n",
    "    \n",
    "\n",
    "    def forward(self, emb_encoder_input, valid_lens, emb_decoder_input=None, embedding=None, phase=1):\n",
    "        encode_result = self.encoder(emb_encoder_input, valid_lens)\n",
    "\n",
    "        # VAE encoding and decoding\n",
    "        vae_encoded = self.vae.encode(encode_result)\n",
    "        vae_sampled = self.vae.reparameterize(*vae_encoded)\n",
    "        vae_decoded = self.vae.decode(vae_sampled)\n",
    "\n",
    "        channel_outputs = self.channel(encode_result)\n",
    "\n",
    "        \n",
    "        if phase == None:\n",
    "            mask = (torch.triu(torch.ones(emb_decoder_input.shape[1], emb_decoder_input.shape[1])) == 1).transpose(0, 1)\n",
    "            mask = (mask.masked_fill(mask == 0, True).masked_fill(mask == 1, False)).to(device)\n",
    "            return encode_result, self.channel(encode_result), \\\n",
    "                   self.decoder(torch.cat([embedding(torch.full([emb_decoder_input.shape[0], 1], source_vocab['<bos>'], dtype=torch.long, device=device)), \n",
    "                                        emb_decoder_input[:, :-1, :]], dim=1).to(device), \n",
    "                                channel_outputs,\n",
    "                                valid_lens, \n",
    "                                mask)\n",
    "        else:\n",
    "            return encode_result, self.channel(encode_result)\n",
    "\n",
    "\n",
    "class MI(nn.Module):\n",
    "    def __init__(self): #6 layers\n",
    "        super(MI, self).__init__()\n",
    "        self.linear1 = nn.Linear(16 * 2, 8 * CONFIG.V)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.linear2 = nn.Linear(8 * CONFIG.V, 4 * CONFIG.V)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.linear3 = nn.Linear(4 * CONFIG.V, 2 * CONFIG.V)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        #self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.linear4 = nn.Linear(2 * CONFIG.V, 2 * CONFIG.V)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        #self.dropout4 = nn.Dropout(p=0.5)\n",
    "        self.linear5 = nn.Linear(2 * CONFIG.V, 2 * CONFIG.V)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        #self.dropout5 = nn.Dropout(p=0.5)\n",
    "        self.linear6 = nn.Linear(2 * CONFIG.V, 1)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        \n",
    "    def network(self, X, Y):\n",
    "        x = self.relu1(self.linear1(torch.cat([X, Y], dim=1)))\n",
    "        #x = self.dropout1(x)\n",
    "        x = self.relu2(self.linear2(x))\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.relu3(self.linear3(x))\n",
    "        #x = self.dropout3(x)\n",
    "        x = self.relu4(self.linear4(x))\n",
    "        #x = self.dropout4(x)\n",
    "        x = self.relu5(self.linear5(x))\n",
    "        #x = self.dropout5(x)\n",
    "        x = self.relu6(self.linear6(x))\n",
    "        return x\n",
    "        \n",
    "    def forward(self, X, Y, valid_lens): \n",
    "        \n",
    "        mask = (torch.arange((X.shape[1]), dtype=torch.long,\n",
    "                            device=X.device)[None, :] >= valid_lens[:, None]).reshape(-1)\n",
    "        # Reshape X and Y first, then take them out\n",
    "        X = X.reshape(-1, 16)\n",
    "        Y = Y.reshape(-1, 16)\n",
    "        \n",
    "        X = X[mask == False]\n",
    "        Y = Y[mask == False]\n",
    "        \n",
    "        # sample\n",
    "        sample_size = X.shape[0]\n",
    "        idx = list(range(sample_size))\n",
    "        random.shuffle(idx)\n",
    "        idx = torch.tensor(idx).to(device)\n",
    "        X = X[idx]\n",
    "        Y = Y[idx]\n",
    "        idx_shuffle = list(range(sample_size))\n",
    "        random.shuffle(idx_shuffle)\n",
    "        idx_shuffle = torch.tensor(idx_shuffle).to(device)\n",
    "        shuffle_Y = Y[idx_shuffle]\n",
    "        \n",
    "        output_joint = self.network(X, Y)\n",
    "        output_marginal = self.network(X, shuffle_Y)\n",
    "        \n",
    "        return output_joint, output_marginal\n",
    "      \n",
    "# Define the driving app\n",
    "def translate(input_file_path, output_log_path, checkpoint_path, batch_size=32, num_steps=40):\n",
    "    # Load the checkpoint\n",
    "    #checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    #source_vocab = checkpoint['source_vocab']\n",
    "    embedding_state_dict = checkpoint['embedding_state_dict']\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "    \n",
    "    # Initialize the embedding and model\n",
    "    embedding = nn.Embedding.from_pretrained(torch.zeros((len(source_vocab), embedding_state_dict['embedding.weight'].shape[1])), freeze=False)\n",
    "    embedding_state_dict['weight'] = embedding_state_dict.pop('embedding.weight')\n",
    "    embedding.load_state_dict(embedding_state_dict)\n",
    "    embedding_state_dict['embedding.weight'] = embedding_state_dict.pop('weight')\n",
    "    model = DeepST().to(device)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Set up the data loader\n",
    "    input_corpus = open(input_file_path, 'r').readlines()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=MyDataset(corpus=input_corpus), batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    num_steps = CONFIG.num_steps\n",
    "    #bleus = []\n",
    "    #output_sentences = []\n",
    "    \n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        val_corpus = open(input_file_path, 'r').readlines()\n",
    "        val_data_loader = torch.utils.data.DataLoader(dataset=MyDataset(corpus=val_corpus), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        \n",
    "        for index, data in enumerate(tqdm(val_data_loader), 0):\n",
    "            inputs, valid_lens = data\n",
    "            inputs, valid_lens = inputs.to(device), valid_lens.to(device)\n",
    "            emb_inputs = embedding(inputs)\n",
    "            _, channel_outputs = model(emb_inputs, valid_lens)\n",
    "            # decoder's first element is <bos>\n",
    "            outputs = torch.cat([torch.full([inputs.shape[0], 1], source_vocab['<bos>'], dtype=torch.long, device=device),\n",
    "                                 torch.full([inputs.shape[0], num_steps - 1], source_vocab['<pad>'], dtype=torch.long, device=device)],\n",
    "                                dim=1).to(device)\n",
    "            # continue_idxtest which sentence can continuously generate\n",
    "            continue_idx = torch.arange(inputs.shape[0], device=device)\n",
    "            num_step = 0\n",
    "            while not len(continue_idx) == 0 and num_step < num_steps - 1:\n",
    "                emb_outputs = embedding(outputs[continue_idx, :num_step + 1])\n",
    "                pred_words = model.decoder(emb_outputs, channel_outputs[continue_idx], valid_lens[continue_idx], mode='validate').argmax(dim=2)[:, -1:]\n",
    "                outputs[continue_idx, num_step + 1] = pred_words.squeeze(1)\n",
    "                continue_idx = continue_idx[(pred_words != source_vocab['<eos>']).squeeze(1)]\n",
    "                num_step += 1\n",
    "     \n",
    "            # Read the output text from file\n",
    "            with open(output_log_path, 'a') as f_log:\n",
    "                for i in range(inputs.shape[0]):\n",
    "                    input_sentence = source_vocab.to_tokens(list(inputs[i].cpu().numpy()))\n",
    "                    input_sentence = ' '.join([t for t in input_sentence if t not in ['<pad>', '<bos>', '<eos>']])\n",
    "                    \n",
    "                    output_sentence = source_vocab.to_tokens(list(outputs[i, 1:].cpu().numpy()))\n",
    "                    output_sentence = ' '.join([t for t in output_sentence if t not in ['<pad>', '<eos>']])\n",
    "                    print(input_sentence)\n",
    "                    print(output_sentence)\n",
    "                    \n",
    "                    output_sentences.append(output_sentence) \n",
    "                    \n",
    "                    \n",
    "                    bleu=0.0\n",
    "                    bleu = sentence_bleu([input_sentence.split()], output_sentence.split(), smoothing_function=SmoothingFunction().method1)\n",
    "                    bleus.append(bleu)\n",
    "                    # Write the input and output sentences and bleu to the output log file\n",
    "                    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    f_log.write('********\\nTimestamp: {}\\nInput: {}\\nOutput: {}\\nBLEU score: {}\\n'.format(timestamp, input_sentence, output_sentence, bleu))\n",
    "\n",
    "            # Append the average BLEU score to the output log file\n",
    "            avg_bleu = sum(bleus) / len(bleus)\n",
    "            with open(output_log_path, 'a') as f_log:\n",
    "                f_log.write('-----------\\n' + f\"Average BLEU score: {avg_bleu}\\n\" + '-----------\\n')\n",
    "            return output_sentences\n",
    "    '''\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_corpus = open(input_file_path, 'r').readlines()\n",
    "        original_order = list(range(len(val_corpus)))\n",
    "        val_data_loader = torch.utils.data.DataLoader(dataset=MyDataset(corpus=val_corpus), batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        output_sentences = [None] * len(val_corpus)  # Initialize output_sentences list with None values\n",
    "        bleus = []\n",
    "        for index, data in enumerate(tqdm(val_data_loader), 0):\n",
    "            inputs, valid_lens = data\n",
    "            inputs, valid_lens = inputs.to(device), valid_lens.to(device)\n",
    "            emb_inputs = embedding(inputs)\n",
    "            _, channel_outputs = model(emb_inputs, valid_lens)\n",
    "            # decoder's first element is <bos>\n",
    "            outputs = torch.cat([torch.full([inputs.shape[0], 1], source_vocab['<bos>'], dtype=torch.long, device=device),\n",
    "                                 torch.full([inputs.shape[0], num_steps - 1], source_vocab['<pad>'], dtype=torch.long, device=device)],\n",
    "                                dim=1).to(device)\n",
    "            # continue_idxtest which sentence can continuously generate\n",
    "            continue_idx = torch.arange(inputs.shape[0], device=device)\n",
    "            num_step = 0\n",
    "            while not len(continue_idx) == 0 and num_step < num_steps - 1:\n",
    "                emb_outputs = embedding(outputs[continue_idx, :num_step + 1])\n",
    "                pred_words = model.decoder(emb_outputs, channel_outputs[continue_idx], valid_lens[continue_idx], mode='validate').argmax(dim=2)[:, -1:]\n",
    "                outputs[continue_idx, num_step + 1] = pred_words.squeeze(1)\n",
    "                continue_idx = continue_idx[(pred_words != source_vocab['<eos>']).squeeze(1)]\n",
    "                num_step += 1\n",
    "\n",
    "            # Read the output text from file and store the translated sentences in the corresponding positions of output_sentences\n",
    "            for i in range(inputs.shape[0]):\n",
    "                original_idx = original_order[index * batch_size + i]\n",
    "                input_sentence = val_corpus[original_idx].strip()\n",
    "\n",
    "                output_sentence = source_vocab.to_tokens(list(outputs[i, 1:].cpu().numpy()))\n",
    "                output_sentence = ' '.join([t for t in output_sentence if t not in ['<pad>', '<eos>', '<bos>']])\n",
    "\n",
    "                output_sentences[original_idx] = output_sentence\n",
    "\n",
    "                # Calculate BLEU score for this sentence\n",
    "                bleu = sentence_bleu([input_sentence.split()], output_sentence.split(), smoothing_function=SmoothingFunction().method1)\n",
    "                bleus.append(bleu)\n",
    "                '''\n",
    "                # Debug statement to track the order of the input sentences\n",
    "                for i in range(inputs.shape[0]):\n",
    "                    original_idx = original_order[index * batch_size + i]\n",
    "                    print(f\"Input sentence {i}: {val_corpus[original_idx].strip()}\")\n",
    "\n",
    "                # Debug statement to track the order of the translated output\n",
    "                for i in range(inputs.shape[0]):\n",
    "                    print(f\"Output sentence {i}: {output_sentences[-inputs.shape[0] + i]}\")\n",
    "                '''\n",
    "                # Write the input and output sentences and BLEU score to the output log file\n",
    "                timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                with open(output_log_path, 'a') as f_log:\n",
    "                    f_log.write('********\\nTimestamp: {}\\nInput: {}\\nOutput: {}\\nBLEU score: {}\\n'.format(timestamp, input_sentence, output_sentence, bleu))\n",
    "\n",
    "\n",
    "        # Calculate and write average BLEU score to output log file\n",
    "        avg_bleu = sum(bleus) / len(bleus)\n",
    "        with open(output_log_path, 'a') as f_log:\n",
    "            f_log.write('-----------\\n' + f\"Average BLEU score: {avg_bleu}\\n\" + '-----------\\n')\n",
    "\n",
    "        return output_sentences    \n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        val_corpus = open(input_file_path, 'r').read()  # read entire file as a string\n",
    "        input_sentences = sent_tokenize(val_corpus)  # split input into sentences using NLTK tokenizer\n",
    "        print(input_sentences)\n",
    "        \n",
    "        output_sentences = []\n",
    "        bleus = []\n",
    "        for input_sentence in input_sentences:\n",
    "            input_tokens = input_sentence.strip().split() + ['<eos>']\n",
    "            input_tensor = torch.tensor(source_vocab[input_tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
    "            valid_lens = torch.tensor([input_tensor.shape[1]], device=device)\n",
    "            emb_inputs = embedding(input_tensor)\n",
    "            _, channel_outputs = model(emb_inputs, valid_lens)\n",
    "            # decoder's first element is <bos>\n",
    "            outputs = torch.full([1, num_steps], source_vocab['<pad>'], dtype=torch.long, device=device)\n",
    "            outputs[0, 0] = source_vocab['<bos>']\n",
    "            # continue_idxtest which sentence can continuously generate\n",
    "            continue_idx = torch.tensor([0], device=device)\n",
    "            num_step = 0\n",
    "            while len(continue_idx) > 0 and num_step < num_steps - 1:\n",
    "                emb_outputs = embedding(outputs[:, :num_step + 1])\n",
    "                pred_words = model.decoder(emb_outputs, channel_outputs, valid_lens, mode='validate').argmax(dim=2)[:, -1:]\n",
    "                outputs[:, num_step + 1] = pred_words.squeeze(1)\n",
    "                continue_idx = (pred_words == source_vocab['<eos>']).nonzero(as_tuple=True)[1]\n",
    "                num_step += 1\n",
    "                if num_step == num_steps - 1:\n",
    "                    break\n",
    "\n",
    "            output_tokens = outputs[0, 1:(num_step+1)]\n",
    "            output_sentence = ' '.join([source_vocab.itos[token] for token in output_tokens.tolist() if source_vocab.itos[token] not in ['<pad>', '<bos>', '<eos>']])\n",
    "            output_sentences.append(output_sentence)\n",
    "\n",
    "            # Calculate BLEU score for this sentence\n",
    "            bleu = sentence_bleu([input_sentence.split()], output_sentence.split(), smoothing_function=SmoothingFunction().method1)\n",
    "            bleus.append(bleu)\n",
    "\n",
    "            # Write input, output, and BLEU score to output log file\n",
    "            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            with open(output_log_path, 'a') as f_log:\n",
    "                f_log.write('********\\nTimestamp: {}\\nInput: {}\\nOutput: {}\\nBLEU score: {}\\n'.format(timestamp, input_sentence, output_sentence, bleu))\n",
    "\n",
    "        # Calculate and write average BLEU score to output log file\n",
    "        avg_bleu = sum(bleus) / len(bleus)\n",
    "        with open(output_log_path, 'a') as f_log:\n",
    "            f_log.write('-----------\\n' + f\"Average BLEU score: {avg_bleu}\\n\" + '-----------\\n')\n",
    "\n",
    "        return output_sentences\n",
    "    '''\n",
    "'''\n",
    "This code defines two endpoints: /translate for starting the translation process, and /results for retrieving the results.\n",
    "\n",
    "When a POST request is sent to the /translate endpoint with the input text in JSON format, the input text is saved to a file, \n",
    "and the translation process is started in the background using a separate process. The endpoint returns a JSON response \n",
    "indicating that the translation has started.\n",
    "\n",
    "When a GET request is sent to the /results endpoint, the output text is retrieved from the output file and returned in a JSON response.\n",
    "'''\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config.from_object(__name__)\n",
    "\n",
    "CONF = {\n",
    "    'input_file_path': 'data/input.txt',\n",
    "    'output_log_path': 'data/log.txt',\n",
    "    'output_file_path': 'data/output.txt',\n",
    "    'checkpoint_path': 'data/model.checkpoint.pth',\n",
    "    'batch_size': 32,\n",
    "    'num_steps': 40\n",
    "}\n",
    "\n",
    "@app.route('/translate', methods=['POST'])\n",
    "def run_translation():   \n",
    "    translated_sentences = []\n",
    "    try:\n",
    "        data = request.get_json(force=True)\n",
    "        input_text = data['input_text']\n",
    "        \n",
    "        \n",
    "        # data preprocessing\n",
    "        # Add an extra space before each punctuation\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', input_text)\n",
    "        # Replace each punctuation with a space plus itself if it is preceded by a word or whitespace\n",
    "        sentences = [re.sub(r'([.,;:!?])(?<=\\1)', r' \\1', sentence) for sentence in sentences]\n",
    "        # Join the sentences back into a single string\n",
    "        output_text = '\\n'.join(sentences)\n",
    "        #print(output_text)\n",
    "        \n",
    "        \n",
    "        # Write the input text to a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, mode='w', newline='') as temp_file:\n",
    "            temp_file.write(output_text)\n",
    "            temp_file_path = temp_file.name\n",
    "        #print(temp_file_path)\n",
    "        \n",
    "        # Translate the input text\n",
    "        translated_sentences = translate(temp_file_path, CONF['output_log_path'], CONF['checkpoint_path'], CONF['batch_size'], CONF['num_steps'])\n",
    "        \n",
    "        # Write the output sentences to file\n",
    "        try:\n",
    "            with open(CONF['output_file_path'], 'w') as f:\n",
    "                for sentence in translated_sentences:\n",
    "                    f.write(sentence)\n",
    "                    f.write('\\n')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {CONF['output_file_path']} not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        # Return a JSON response indicating that the translation has been completed\n",
    "        with open(CONF['output_file_path'], 'r') as f:\n",
    "            output_text = f.read()\n",
    "        return jsonify({'message': 'Translation successful.', 'status': 'success', 'output_text': output_text})\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({'message': 'Error during translation', 'status': 'error'})\n",
    "\n",
    "\n",
    "@app.route('/results', methods=['GET'])\n",
    "def get_results():\n",
    "    try:\n",
    "        with open(CONF['output_file_path'], 'r') as f:\n",
    "            output_text = f.read()\n",
    "\n",
    "        # Define a regular expression pattern to match one or more spaces before punctuation\n",
    "        pattern = r'\\s*([^\\w\\s]+)\\s*'\n",
    "        # Use the sub() function to replace all matches of the pattern with the punctuation character     \n",
    "        output_text = re.sub(pattern, r'\\1 ', output_text)\n",
    "        \n",
    "        return jsonify({\n",
    "            'message': 'Results retrieved successfully.',\n",
    "            'status': 'success',\n",
    "            'output_text': '{}'.format(output_text)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({'message': 'Error retrieving results.', 'status': 'error'})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5555)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
